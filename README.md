FIX CITATIONS IN DATA AND REFERENCE SECTION 
Exploring the Role of Data Analysis Within the Metaliteracy Framework
by A. Brignole
A Project for the Coursera Course “Empowering Yourself in a Post-Truth World”
Professors:  T. P. Mackey, Ph.D., K. O’ Brien, and T. Jacobsen

Professor’s Stated Project Goal:
Through the creation and sharing of your final project, realize your vision for how you would reinvent a truthful world as an empowered metaliterate learner. Reflect on how you would apply the lessons learned in this MOOC to address the challenges of the post-truth world. Imagine how we would work together to effectively build a new future of truth and reason based on interrelated communities of trust. Think about how we move past the use of social media and emerging technologies to simply connect individuals in order to build online communities in which participants collaborate in purposeful ways to solve problems and make meaning. Reflect on what is required to move from post-truth circumstances to a future built on truth, trust, and both individual and community empowerment.

Project Instructions:
1.)	 Include the hyperlink to the project
Hyperlink:  
2.)	Explain why you chose this topic.
Knowledge stems from information which ultimately derives from data (Kempe, 2013).  Then, if experts possess knowledge, the information yielding knowledge should ultimately be supported by data.
3.)	Describe the audience for your project
My audience is any individual interested in data as it relates to information, knowledge, expertise, information literacy and metaliteracy
4.)	What considerations went into your selection of the tool for this project
Datum from research is an important tool.  It serves as the building block for information and knowledge.  Replicability of research findings are important in establishment of scientific theories
5.)	Relate this project creation experience to the metaliteracy learning domains
Data is sometimes given a free-pass as ground truth; however, data may be unique to the conditions of data collection.  Moreover, the Metaliteracy paradigm lends itself to critical review bias, credibility, and reliability within research methods, data analysis, and conclusions drawn from research.  
6.)	Credit all external sources
(see Reference Citations)

Project
Introduction
Data are the foundation for information, and find use in establishment of knowledge (Kempe, 2013), and measurements of uncertainty (e.g. statistics).  Then, the reliability and credibility of sources (such as the news media, or those considered experts) hinges on support from data, the data collection methods, the analytical methods, and the conclusions drawn from the analysis.  Familiarity with data, its analysis, interpretations, and conclusions make experts reliable sources of information.  As the documented body of human knowledge increases and becomes ever-granular, our dependence on experts may increase, as well.  
We live in a world that is profoundly shaped by the opinions of it’s constituents.  Ensuring that the individuals who make decisions (whether government officials, voters, or other stakeholders [participants]) have the breadth of knowledge to make informed decisions is paramount to producing improved outcomes for the world-at-large.  In contrast, absence of knowledge, and bias may result in misleading presentation of facts, and faulty decision making, where outcomes would deviate from those that would occur under the presence of such information. 
Mackey and Jacobsen (2011) describe a theoretical framework called “Metaliteracy” that lends itself to critically assessing information literacy (and subsequently source data) with consideration to a.) the inner workings of the supported information itself (a cognitive domain), b.) the potential shortcomings of the information (a metacognitive domain), c.) the information’s influence from and on emotive response (an affective domain), and d.) implications as the information is implemented into various forms of communications and engineering (a behavioral domain).  Put into practice, Jacobsen, Mackey, and O’ Brien, et al (2018) outline the following four activities as goals for their Metaliteracy Framework:
1.	Evaluate content while recognizing one’s own bias.
2.	Engage with intellectual property ethically and responsibly.
3.	Produce and share information in a collaborative and participatory environment
4.	Develop learning strategies that meet lifelong personal goals. (Jacobsen, Mackey, and O’ Brien, et al [2018])

Importance of Collected and Studied Data 
While Mackey, O’Brien, and Jacobsen (XXXX) stress the importance of experts.  It is important to know whether an individual can boast such status.  Mackey, O’Brien, and Jacobsen (xxxx) turn to the National Academy of Sciences’ National Research Council (NRC) which offer the following six characteristics they note of experts:
1.	Experts notice features and meaningful patterns of information not noticed by novices
2.	Experts have acquired a great deal of knowledge that is organized in ways that reflect a deep understanding of their subject
3.	Expert knowledge reflects contexts of applicability; that is, the knowledge is conditionalized to a set of circumstances
4.	Experts are able to flexibly retrieve important aspects of their knowledge with little attentional effort
5.	Experts know a lot, but may not be effective teachers
6.	Experts have varying levels of flexibility in their approach to new situations. (NRC, 1999) 
The NRC definition may be purposefully ambiguous.  However, the NRC definition incorporates knowledge but ignores the very root concept from which knowledge derives.  The Data-Information-Knowledge Lifecycle (Kempe, 2013) illustrates that the very root of knowledge is actually data.  Then, an expert’s argument is only as credible as data and analysis that they reference.  It would be unfortunate if an expert offered only opinion without the support of data.  In addition, the data that is cited should be understood to be of reputable quality.  One common aphorism among data analysts is “Garbage in:  garbage out”; meaning if the data is of poor quality, then conclusions from the source data will also be of poor quality.  
Another criticism with the NRC definition is that the definition of an expert places focus on the characteristics of an individual.  Instead, the opinions and conclusions of an expert should be validated by the community within which they have been deemed an expert.  Perhaps, the democratization of expertise hinders the minority of experts that deviate from mainstream opinions; however, science reproducibility and replicability (as defined by Patil, Peng, and Leek, 2016), and affirmation of study methods, analysis and conclusions are paramount in establishing credible theories.  In their book Noise, Kahneman, Sibony, and Sunstein (2020, pp. 111-158) cite numerous studies suggesting data-based algorithms and rule-based models can outperform individual judgment.  
Then, a substantiated argument can be made that the data is as important, if not more so, than the opinion of any given expert.  For one, an expert opinion is the opinion of one individual; whereas data can transcend the analysis and conclusions of an individual.  Furthermore, study methods, analysis, and conclusions should theoretically result in reproducible and replicable findings which then could support conclusions regardless of the individual performing the study, analysis, and or conclusions.  

No Data, Without Skepticism
One common misunderstanding is that data represent an infallible representation of Truth.  The value of data is that it offers as a measure that is testable and repeatable; and if reported honestly, data serve the goal of an objective representation of whatever it seeks to measure.  Moreover, data may be shared with reviewers for scrutiny and review – the third stated goal of metaliteracy (Mackey and Jacobsen, 2018).  Statistics as a branch of data science and mathematics is the study of uncertainty that exists within data.  
Study design, study methods, the collected data itself, the data analysis, or the conclusions drawn from the data may contain bias – the first stated goal of metaliteracy (Mackey and Jacobsen, 2018) – and should be addressed as part of any analysis, and reported along with any conclusions.  
Bias within the data itself
Within the data itself, bias can take several forms.  Sampling selection bias may occur when the selection of units from the population are not selected at random (Lohr, 2010, p. 5).  Data may also be biased when the sample size is insufficient to expect that the statistics will offer a good measure of population parameters.  Presence of outliers or skewness in the underlying population or within a sample may contribute to poor characterization of a population from small samples of data.  Another bias may arise from individual’s personal biases which may affect the collection of study results.   Lastly, fraud could take the form of fabricated data, in which case there is no expectation that the results are valid.  
Bias within the study methods
Study methods and design could be another source of bias.  Survey questionnaires may be authored to sway survey participants to respond in a certain fashion.  Lohr (2010, p. 14) refers to these questions as leading and loaded questions.  Study methods from a sample unrepresentative of the population they intended to describe would be another source of bias.  
Study method bias differs from sample selection bias in that the units would not be an otherwise eligible member of the population sampling frame, at large.  
Bias within the Analysis
The most common statistical tests are null hypothesis statistical tests which require a pre-established acceptable false positive rate as a level for establishing “statistical significance”.  Critique of these “p-values” drove the American Statistical Association to devote an entire publication of nearly two-dozen papers critically on input from various points of view (The American Statistician, 2016) and convene a President’s task force to advise on continued and future use (Benjamini, De Veaux, Efron, et al, 2021).  Selection of a false positive rate too permissive, may result in just that – a false positive finding.  False positives might also derive from applying lower powered statistical tests, p-hacking datasets (whoever coined this term, whenever), or failing to meet criteria of severe statistical testing (Mayo, 2018).  One criticism of Bayesian statistical analyses is the use of a prior probability, to which the results are then applied.

Drawing Erroneous Conclusions from Data
Confirmation bias may lead to overconfidently stated conclusions.  It seems that a single study or publication would or should not result in definitive conclusions, especially prior to scrutiny of other experts in the field.  Still, a seminal work in the field is the goal of many a publication.  Several prestigious journals require submissions to specifically inform why the work is a notable advancement in the field before it is considered for publication.  Such practice may lead to “jumping to conclusions” well outside of the scope of a study.  Could a study informing of the benefits of drinking coffee, really mean that physicians should start promoting patients’ consumption of the beverage to prevent cancers?  Could a study of individual behavior in a controlled environment have broader implications for how an economy runs?  Conclusions such of these may ignore the many factors that may play a role in how our world works.  


Reference Citations:

•	American Statistical Association.  (2016).  The American Statistician:  70(2).  https://amstat.tandfonline.com/doi/full/10.1080/00031305.2016.1154108#.Vt2XIOaE2MN

•	Benjamini, Y., De Veaux, R., Efron, B., Evans, S., Glikman, M., Graubard, B. I. He, X., Meng, X., Reid, N., Stigler, S. M., Vardeman, S. B., Wikle, C. K., Wright, T., Young, L. J., and Kafadar, K. (2021, September).  The ASA [American Statistical Association] President’s task force on statistical significance and replicability.  The Annals of Applied Statistics. 13(5), pp 1084-1085   Doi:10.1214/21-AOAS1501

•	Jacobsen, T. Mackey, T. P., O’Brien, K. Forte, M., and O’Keeffe, E. (2018).  2018 metaliteracy goals and objectives.  Metaliteracy.org Accessible from https://metaliteracy.org/learning-objectives/2018-metaliteracy-goals-and-learning-objectives/

•	Kahneman, D., Sibony, O., and Sunstein, C. R.  (2020).  Noise:  A Flaw in Human Judgment.  New York:  Little, Brown Spark.  ISBN 978-0-316-45140-6.  http://www.littlebrownspark.com

•	Kempe.  S.  (2013, November 14).  The data – information – knowledge lifecycle.  Dataversity.  Accessed October 16, 2021 from https://www.dataversity.net/the-data-information-knowledge-cycle/

•	Lohr, S.  (2010).  Sampling Design and Analysis, 2nd Edition.  Boston, MA:  Brooks/Cole Cengage Learning.  

•	Mackey, T. P., and Jacobsen, T. E.  (2011, January).  Reframing information literacy as a metaliteracy.  College and Research Libraries.  

•	Mackey, T. P., O’ Brien, K., and Jacobsen, T.  (XXXXX).  Empowering Yourself in a Post-Truth World.  Coursera.

•	Mackey, T. P. and Jacobsen, T.  ().  Metaliteracy Blog.

•	Mackey, T. P. and Jacobsen, T.  ().  ML in practice.  Metaliteracy.org.  Accessed on October 16, 2021 from  https://metaliteracy.org/ml-in-practice/

•	Mayo, D.  (2018).  Statistical Inference as Severe Testing:  How to Get Beyond the Statistics Wars.  Cambridge, MA.  Cambridge University Press.  ISBN:  978-1-107-05413

•	Patil, P., Peng, R. D., and Leek, J. T. (2016, July 29).  A statistical definition for reproducibility and replicability. (Journal preprint)  bioRxiv.  DOI: 10.1101/066803.  Accessed October 17, 2021 from https://www.biorxiv.org/content/biorxiv/early/2016/07/29/066803.full.pdf

•	Wikipedia.  Draft:  Metaliteracy.  Accessed on October 16, 2021 from https://en.wikipedia.org/wiki/Draft:Metaliteracy

•	Wikipedia.  “Information”.  Accessed on October 16, 2021 from https://en.wikipedia.org/wiki/Information


